{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended spec data cleaning \n",
    "- ml.m7i.8xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install tokenizers transformers ipywidgets pandas datasets wandb huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install accelerate -U\n",
    "# ! pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws s3 sync s3://monolingual.data/A/ /home/ec2-user/SageMaker/monolingual/A/ --no-sign-request\n",
    "\n",
    "# aws s3 sync s3://openpecha.cleaned/tokenized_raw_text/ /home/ec2-user/SageMaker/monolingual/gold/ --no-sign-request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/ec2-user/SageMaker/cache/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo $HF_HOME\n",
    "!echo $HF_DATASETS_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "from huggingface_hub import HfFolder, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('spsither/tibetan_monolingual_A', cache_dir=\"/home/ec2-user/SageMaker/cache/datasets\", num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_transcription(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.strip()\n",
    "    \n",
    "    text = re.sub(\"༌\", \"་\",text) # there are two type of 'tsak' let's normalize 0xf0b to 0xf0c\n",
    "    text = re.sub(\"༎\", \"།\",text) # normalize double 'shae' 0xf0e to 0xf0d\n",
    "    \n",
    "    text = re.sub(\"ཽ\", \"ོ\",text) # normalize\n",
    "    text = re.sub(\"ཻ\", \"ེ\",text) # normalize \"᫥\"\n",
    "    \n",
    "    text = re.sub(r\"\\s+།\", \"།\", text)\n",
    "    text = re.sub(r\"།+\", \"།\", text)\n",
    "    text = re.sub(r\"།\", \"། \", text)\n",
    "    text = re.sub(r\"\\s+་\", \"་\", text)\n",
    "    text = re.sub(r\"་+\", \"་\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"ཧཧཧ+\", \"ཧཧཧ\", text)\n",
    "    text = re.sub(r'ཧི་ཧི་(ཧི་)+', r'ཧི་ཧི་ཧི་', text)\n",
    "    text = re.sub(r'ཧེ་ཧེ་(ཧེ་)+', r'ཧེ་ཧེ་ཧེ་', text)\n",
    "    text = re.sub(r'ཧ་ཧ་(ཧ་)+', r'ཧ་ཧ་ཧ་', text)\n",
    "    text = re.sub(r'ཧོ་ཧོ་(ཧོ་)+', r'ཧོ་ཧོ་ཧོ་', text)\n",
    "    text = re.sub(r'ཨོ་ཨོ་(ཨོ་)+', r'ཨོ་ཨོ་ཨོ་', text)\n",
    "\n",
    "    chars_to_ignore_regex = \"[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\/\\{\\}\\(\\)༽》༼《༅༄༈༑༠'|·×༆༔༷༸༾ཿ྄྅྆྇ྋ࿒ᨵ​’„╗᩺╚༿᫥ྂ༊ྈ࿄࿉࿐྾༜]\"\n",
    "    text = re.sub(chars_to_ignore_regex, '', text)+\" \"\n",
    "    return text\n",
    "print(clean_transcription('ཧིཧོ་ཧོ་ཧོ་ཧོ་ཧོ་ཧོ་  ཧ་ ཧ་ཧ་། །  འ་༽འ་××འ༌༌༌༌༌༌༌གྲོ།ཚ ར་སོང�་ངེ་། '))\n",
    "\n",
    "print(clean_transcription('༼ཕ་༽། སེང་གེ་སྒྲའི་སྒྲུབ་ཐབས་བཞུགས། ། བླ་མ་དམ་པའི་ཞབས་ལ་ཕྱག་འཚལ་ལོ།'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "delimiters = \"་། \"\n",
    "pattern = f\"[{re.escape(delimiters)}]+\"\n",
    "    \n",
    "def max_char_btw_tsak(example):\n",
    "    segments = re.split(pattern, example)\n",
    "    # print([(len(segment), segment) for segment in segments if segment])\n",
    "    max_length = max([len(segment) for segment in segments if segment], default=0)\n",
    "    return max_length\n",
    "\n",
    "def merge_text_lines(examples):\n",
    "    examples = examples['text']\n",
    "    examples = [clean_transcription(example) for example in examples]\n",
    "    \n",
    "    char_lens = [ len(example) for example in examples]\n",
    "    \n",
    "    max_lengths = [ max_char_btw_tsak(example) for example in examples]\n",
    "    \n",
    "    return {'text': examples, 'char_len': char_lens, 'max_char_btw_tsak': max_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset_meta = dataset.map(merge_text_lines, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_meta.push_to_hub('tibetan_monolingual_A_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('spsither/tibetan_monolingual_A_meta', cache_dir=\"/home/ec2-user/SageMaker/cache/datasets\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a filter function\n",
    "def filter_condition(examples):\n",
    "    return [ max_char_btw_tsak > 1 and max_char_btw_tsak < 9 and char_len > 15 and char_len < 1000 for max_char_btw_tsak,char_len in zip(examples['max_char_btw_tsak'], examples['char_len'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "filtered_dataset = dataset.filter(filter_condition, batched = True, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_dataset.push_to_hub('tibetan_monolingual_A_filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('spsither/tibetan_monolingual_A_filtered', cache_dir=\"/home/ec2-user/SageMaker/cache/datasets\", num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"char_len\", 'max_char_btw_tsak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hash(example):\n",
    "    \"\"\"Get hash of content field.\"\"\"\n",
    "    return {\"hash\": hashlib.md5(example[\"text\"].strip().encode(\"utf-8\")).hexdigest()}\n",
    "\n",
    "def check_uniques(example, uniques):\n",
    "    \"\"\"Check if current hash is still in set of unique hashes and remove if true.\"\"\"\n",
    "    if example[\"hash\"] in uniques:\n",
    "        uniques.remove(example[\"hash\"])\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"Chain all preprocessing steps into one function to not fill cache.\"\"\"\n",
    "    results = dict()\n",
    "    results.update(get_hash(example))\n",
    "    return results\n",
    "\n",
    "def filter(example, uniques):\n",
    "    \"\"\"Filter dataset with heuristics. Config, test and has_no_keywords files are removed with a given probability.\"\"\"\n",
    "    if not check_uniques(example, uniques):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "dataset = dataset.map(preprocess, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deduplicate hashes\n",
    "uniques_train = set(dataset['train'][\"hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deduplicate data and apply heuristics\n",
    "ds_filter_train = dataset['train'].filter(filter, fn_kwargs={\"uniques\": uniques_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uniques_test = set(dataset['test'][\"hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deduplicate data and apply heuristics\n",
    "ds_filter_test = dataset['test'].filter(filter, fn_kwargs={\"uniques\": uniques_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "deduped_filtered_dataset = DatasetDict()\n",
    "\n",
    "deduped_filtered_dataset['train'] = ds_filter_train\n",
    "deduped_filtered_dataset['test'] = ds_filter_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deduped_filtered_dataset = deduped_filtered_dataset.remove_columns([\"hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "deduped_filtered_dataset.push_to_hub('tibetan_monolingual_A_filtered_deduped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the filtered_dataset as files on disk and train BPE tokenizer\n",
    "deduped_filtered_dataset['test'].to_csv('deduped_filtered_dataset_test')\n",
    "deduped_filtered_dataset['train'].to_csv('deduped_filtered_dataset_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -n +2 deduped_filtered_dataset_test > tmp.csv && mv tmp.csv  /home/ec2-user/SageMaker/monolingual/A_filtered_deduped/deduped_filtered_dataset_test.csv\n",
    "! tail -n +2 deduped_filtered_dataset_train > tmp.csv && mv tmp.csv /home/ec2-user/SageMaker/monolingual/A_filtered_deduped/deduped_filtered_dataset_train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tibetan_monolingual_A_filtered_deduped doesn't need to be merged to form paragraphs. Some sentences are 1000 char long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_text_lines(examples):\n",
    "    # print(examples)\n",
    "    examples = [example.strip() for example in examples['text']]\n",
    "    merged_examples = []\n",
    "    total_examples = len(examples)\n",
    "    first_third_point = total_examples // 3  # End of the first third\n",
    "    second_third_point = 2 * total_examples // 3  # End of the second third\n",
    "    \n",
    "    # print(total_examples, first_third_point, second_third_point)\n",
    "    for i in range(total_examples):\n",
    "        if i < first_third_point:\n",
    "            step = 1  # Done merge in the first third\n",
    "        elif i < second_third_point:\n",
    "            step = 2  # Merge every 2 lines in the second third\n",
    "        else:\n",
    "            step = 4  # Merge every 4 lines in the last third\n",
    "\n",
    "        # Check if the current index is a starting point for a new group\n",
    "        if i % step == 0:\n",
    "            # Prevent going beyond the list length\n",
    "            merged_examples.append(' '.join(examples[i:i+step]))\n",
    "\n",
    "    return {'text': merged_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "filtered_merged_dataset = deduped_filtered_dataset.map(merge_text_lines, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_merged_dataset.push_to_hub('tibetan_monolingual_A_filtered_deduped_merged_124_lines')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
