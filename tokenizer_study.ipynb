{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended spec data cleaning \n",
    "- ml.m7i.8xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install tokenizers transformers ipywidgets pandas datasets wandb huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install accelerate -U\n",
    "# ! pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "from huggingface_hub import HfFolder, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "paths = [str(x) for x in Path(\"/home/ec2-user/SageMaker/monolingual\").glob(\"A_filtered_deduped/*.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# openpecha/Madlad-v1 has 256000. sangjeedondrub/tibetan-roberta-base has 52000.\n",
    "# when I set it to be 52000 BPE generated 52000 tokens\n",
    "# when I set it to be 256000 BPE generated 86761 tokens\n",
    "# 86761 seems optimal cos the tokenizer training step uses 86761 even if it has option to generate more for min_frequency 2.\n",
    "# Looking at tokenizer_G using 86761, there are too many nonsensical tokens and tokenizer A with vocab size 52000 has fewer of those.\n",
    "\n",
    "VOCAB_SIZE = 256000\n",
    "MAX_LEN    = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=VOCAB_SIZE, min_frequency=2, # tried 1 gives more tokens, try 3 may give better tokens\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the Tokenizer to disk\n",
    "tokenizer_folder = 'tokenizer_A_f_d'\n",
    "! mkdir {tokenizer_folder}\n",
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_folder = 'tokenizer_A_f_d'\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the tokenizer\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# add_prefix_space=True\n",
    "# Set this when you want to tokenizer to work with syllables using text.split('་'). Useful for NER/POS/Word Chuncking. \n",
    "# use is_split_into_words=True when calling tokenizer to use this\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "md_tokenizer = AutoTokenizer.from_pretrained(\"google/madlad400-3b-mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_tokens_dict = {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"sep_token\": '</s>', \"pad_token\": '<pad>', \"cls_token\":'<s>', \"mask_token\":'<mask>'}\n",
    "md_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "md_tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_id = [118443,256000]  # The ID of the beginning-of-sentence token\n",
    "tok = md_tokenizer.decode(token_id, skip_special_tokens=False)\n",
    "print(f\"_{tok}-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'ཁ་ཤས་སོ་སོས་དོ་སྣང་བྱས་ཏེ་ཉམས་ལེན་བྱེད་མཁན་དེ་འདྲ་ཡོང་གི་འདུག་གང་ལྟར་སྔོན་མ་ཡིན་ན་ཆོས་ཁྲིམས་པས་སྐོར་བ་བརྒྱབ་བྱས། མ་སྡོད་ཉལ་ནས་སྡོད་ན་ཆད་པ་ཡིན།'\n",
    "print(len(text), len(tokenizer(text)['input_ids']))\n",
    "\n",
    "print(tokenizer(text)['input_ids'])\n",
    "'_'.join([tokenizer.decode(i) for i in tokenizer(text)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(text), len(md_tokenizer(text)['input_ids']))\n",
    "\n",
    "print(md_tokenizer(text)['input_ids'])\n",
    "'_'.join([md_tokenizer.decode(i) for i in md_tokenizer(text)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(text), len(md_tokenizer(text)['input_ids'])\n",
    "md_tokenizer.decode(md_tokenizer(text)['input_ids'])\n",
    "print(md_tokenizer(text, padding='max_length', truncation=False, max_length=100)['input_ids'])\n",
    "'_'.join([md_tokenizer.decode(i, skip_special_tokens=False) for i in md_tokenizer(text)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('/home/ec2-user/SageMaker/RoBERTa/tokenizer_A_f_d/vocab.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All BPE tokens\n",
    "for k in data:\n",
    "    print(tokenizer.decode(data[k]), sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
