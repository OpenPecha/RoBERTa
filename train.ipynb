{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer this [blog](https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c) and this [code](https://github.com/edumunozsala/RoBERTa_Encoder_Decoder_Product_Names/blob/03c0456f03d8cff62e2d1b04f03029130694e18b/RoBERTa%20MLM%20and%20Tokenizer%20train%20for%20Text%20generation.ipynb)\n",
    "\n",
    "\n",
    "Recommended spec for training\n",
    "- ml.g5.4xlarge\n",
    "- ml.g5.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install tokenizers transformers ipywidgets pandas datasets wandb huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install accelerate -U\n",
    "# ! pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws s3 sync s3://monolingual.data/A/ /home/ec2-user/SageMaker/monolingual/A/ --no-sign-request\n",
    "\n",
    "# aws s3 sync s3://openpecha.cleaned/tokenized_raw_text/ /home/ec2-user/SageMaker/monolingual/gold/ --no-sign-request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/ec2-user/SageMaker/cache/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo $HF_HOME\n",
    "!echo $HF_DATASETS_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168290a4f2364037b09a7f9514333562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "from huggingface_hub import HfFolder, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "dataset = load_dataset('spsither/tibetan_monolingual_S_cleaned_train_test', num_proc=24)\n",
    "\n",
    "VOCAB_SIZE = 52000\n",
    "MAX_LEN = 512\n",
    "tokenizer_folder = \"tokenizer_S_b\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, return_tensors=\"pt\", return_special_tokens_mask=True, return_special_tokens_mask=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "tokenized_dataset.push_to_hub('spsither/tibetan_monolingual_S_cleaned_train_test_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# openpecha/Madlad-v1 has 256000. sangjeedondrub/tibetan-roberta-base has 52000.\n",
    "# when I set it to be 52000 BPE generated 52000 tokens\n",
    "# when I set it to be 256000 BPE generated 86761 tokens\n",
    "# 86761 seems optimal cos the tokenizer training step uses 86761 even if it has option to generate more for min_frequency 2.\n",
    "# Looking at tokenizer_G using 86761, there are too many nonsensical tokens and tokenizer A with vocab size 52000 has fewer of those.\n",
    "\n",
    "VOCAB_SIZE = 52000\n",
    "MAX_LEN    = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_folder = 'tokenizer_S_b'\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# Set a configuration for our RoBERTa model\n",
    "config = RobertaConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_LEN + 2,\n",
    "    num_attention_heads=12, #  12 Medium | 16 Large\n",
    "    num_hidden_layers=6,    #   6 Medium | 24 Large\n",
    "    hidden_size=768,        # 768 Medium | 1024 Large\n",
    "    type_vocab_size=1\n",
    ")\n",
    "# Initialize the model from a configuration without pretrained weights\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('Num parameters: ',model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# add_prefix_space=True\n",
    "# Set this when you want to tokenizer to work with syllables using text.split('་'). Useful for NER/POS/Word Chuncking. \n",
    "# use is_split_into_words=True when calling tokenizer to use this\n",
    "tokenizer_folder = 'tokenizer_S_b'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('spsither/tibetan_monolingual_S_cleaned_train_test', cache_dir=\"/home/ec2-user/SageMaker/cache/datasets\", num_proc=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what ever value you see for vocab_size, consider using that for VOCAB_SIZE. i.e. the least required value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len=MAX_LEN):\n",
    "        self.df = pd.DataFrame(dataset['text'])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inputs = self.tokenizer.encode_plus(self.df.iloc[i, 0],\n",
    "                                       max_length=self.max_len,\n",
    "                                       truncation=True,\n",
    "                                       padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "\n",
    "        return {'input_ids': inputs.input_ids[0], 'attention_mask': inputs.attention_mask[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset = CustomDataset(dataset['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(dataset['train'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training\n",
    "# from transformers import RobertaForMaskedLM\n",
    "# model = RobertaForMaskedLM.from_pretrained('/media/monlamai/SSD/RoBERTa/checkpoint-480288')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "# Batch size of 60 worked on ml.g5.4xlarge with gradient_checkpointing & fp16 True. group_by_length takes extra time.\n",
    "# Running on ml.g5.12xlarge uses Data Parallelism on 4gpus \n",
    "# but batch size it takes is 8 at max thus not making it worth the additional cost. Also not sure if it actually works.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '/media/monlamai/SSD/RoBERTa',\n",
    "    overwrite_output_dir = False,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    num_train_epochs = 50,         \n",
    "    learning_rate = 1e-4,          # default: 0.001\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size = 60, # 59-61 $ 60-1gpu TTF # 30-1gpu FFF # 4-4gpus TTT \n",
    "    per_device_eval_batch_size  = 60, # can be larger than per_device_train_batch_size, no need for grad\n",
    "    gradient_checkpointing = True,   # default False Saves a lot of mem\n",
    "    fp16                   = True,   # default False Saves some mem\n",
    "    group_by_length        = False,  # default False # takes time\n",
    "    gradient_accumulation_steps = 1, # default 1\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 100,\n",
    "    save_total_limit = 40,\n",
    "    report_to = ['wandb'],\n",
    ")\n",
    "# Create the trainer for our model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=eval_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"parallel_mode: {training_args.parallel_mode} \\nn_gpus: {training_args.n_gpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=True)  # continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save our tokenizer and create a model card\n",
    "repository_id = 'spsither/tibetan-RoBERTa'\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.config.to_json_file(f\"{tokenizer_folder}/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "tokenizer_folder = 'tokenizer_f_d'\n",
    "MAX_LEN    = 512\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)\n",
    "model = RobertaForMaskedLM.from_pretrained('/home/ec2-user/SageMaker/RoBERTa/checkpoint-1445231')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('tibetan_RoBERTa_Afd_e1')\n",
    "tokenizer.push_to_hub('tibetan_RoBERTa_Afd_e1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=f\"{model_folder}/checkpoint-110120\",\n",
    "    tokenizer=tokenizer_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          སེམས་ཀྱི་རང་བཞིན་འོད་གསལ་བ་ཟེར་ཡ་དེ་\n",
    "fill_mask(\"སེམས་ཀྱི་རང་བཞིནའོད་<mask>་བ་ཟེར་ཡ་དེ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = \"\"\"རིན་ <mask>\n",
    "ཆོས་ཀྱི་ <mask>\n",
    "རྫོགས་པའི་ <mask>\n",
    "གངས་རིའི་ <mask>\n",
    "མེ་ལོང་ <mask>\n",
    "བདེན་པའི་ <mask>\n",
    "'འབྱུང་ <mask>\"\"\".splitlines()\n",
    "\n",
    "for idx, sample in enumerate(samples, start=1):\n",
    "    outputs = fill_mask(sample)\n",
    "    print(idx, sample)\n",
    "    for output in outputs:\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"དེ་ནས་ཤར་ཕྱོགས་སུ་ནགས་སྟུག་པོ་བརྒྱུད་དེ་རྒྱང་གྲགས་ཉིས་བརྒྱ་བགྲོད་པ་ན་ཨི་ར་ན་བྷ་ཏའི་ཡུལ་ལོ། །ཐ་གྲུར་རྒྱང་གྲགས་སུམ་སྟོང་ལྷག་པ། ལྟེ་བའི་མཁར་ཆེན་ནི།\"\n",
    "ground = text.split('་')\n",
    "corrects = []\n",
    "for i in range(len(ground)):\n",
    "    test = ground[::]\n",
    "    mask = test[i]\n",
    "    test[i] = '<mask>'\n",
    "    test = '་'.join(test)\n",
    "    infs = fill_mask(test)\n",
    "    correct = False\n",
    "    for inf in infs:\n",
    "        if inf['token_str'] == mask:\n",
    "            correct = True\n",
    "    corrects += [correct]\n",
    "    \n",
    "print(sum(corrects), len(corrects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "tokenizer_folder = 'tokenizer_S_b'\n",
    "MAX_LEN    = 512\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)\n",
    "model = RobertaForMaskedLM.from_pretrained('/home/ec2-user/SageMaker/RoBERTa/RoBERTa/checkpoint-480288')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('tibetan_RoBERTa_S_e6')\n",
    "tokenizer.push_to_hub('tibetan_RoBERTa_S_e6')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
