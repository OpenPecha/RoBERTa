{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer this [blog](https://medium.com/analytics-vidhya/create-a-tokenizer-and-train-a-huggingface-roberta-model-from-scratch-f3ed1138180c) and this [code](https://github.com/edumunozsala/RoBERTa_Encoder_Decoder_Product_Names/blob/03c0456f03d8cff62e2d1b04f03029130694e18b/RoBERTa%20MLM%20and%20Tokenizer%20train%20for%20Text%20generation.ipynb)\n",
    "\n",
    "\n",
    "Recommended spec for training\n",
    "- ml.g5.4xlarge\n",
    "- ml.g5.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install tokenizers transformers ipywidgets pandas datasets wandb huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install accelerate -U\n",
    "# ! pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/ec2-user/SageMaker/cache/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo $HF_HOME\n",
    "!echo $HF_DATASETS_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# 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\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "from huggingface_hub import HfFolder, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "paths = [str(x) for x in Path(\"/home/ec2-user/SageMaker/monolingual\").glob(\"A/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# openpecha/Madlad-v1 has 256000. sangjeedondrub/tibetan-roberta-base has 52000.\n",
    "# when I set it to be 52000 BPE generated 52000 tokens\n",
    "# when I set it to be 256000 BPE generated 86761 tokens\n",
    "# 86761 seems optimal cos the tokenizer training step uses 86761 even if it has option to generate more for min_frequency 2.\n",
    "# Looking at tokenizer_G using 86761, there are too many nonsensical tokens and tokenizer A with vocab size 52000 has fewer of those.\n",
    "\n",
    "VOCAB_SIZE = 52000\n",
    "MAX_LEN    = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=VOCAB_SIZE, min_frequency=2, # tried 1 gives more tokens\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the Tokenizer to disk\n",
    "! mkdir tokenizer\n",
    "tokenizer_folder = 'tokenizer'\n",
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_folder = 'tokenizer'\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the tokenizer\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# Set a configuration for our RoBERTa model\n",
    "config = RobertaConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,     # 16 Large, 12 Medium\n",
    "    num_hidden_layers=6,        # 24 Large, 6 Medium\n",
    "    type_vocab_size=1,\n",
    "    hidden_size=768             # 1024 Large, 768 Medium\n",
    ")\n",
    "# Initialize the model from a configuration without pretrained weights\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('Num parameters: ',model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# add_prefix_space=True\n",
    "# Set this when you want to tokenizer to work with syllables using text.split('་'). Useful for NER/POS/Word Chuncking. \n",
    "# use is_split_into_words=True when calling tokenizer to use this\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths_train = paths[0:53000]\n",
    "paths_test  = paths[53000:]\n",
    "# paths_train = paths[0:100]\n",
    "# paths_test  = paths[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(paths_test) / len(paths) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the text files as a dataset\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": paths_train, \"test\": paths_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"spsither/tibetan_monolingual_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTEXT_LINES = 7 # try 5 next time some sentences are very long, also some mixture of fewer sentences\n",
    "def merge_text_lines(examples, context_lines = CONTEXT_LINES):\n",
    "    examples = examples['text']\n",
    "    # print(examples)\n",
    "    merged_examples = []\n",
    "    for i in range(0, len(examples), context_lines):\n",
    "        merged_examples.append(' '.join(examples[i:i+context_lines]))\n",
    "    return {'text' : merged_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "merged_dataset = dataset.map(merge_text_lines, batched=True, batch_size=CONTEXT_LINES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_dataset.push_to_hub(\"spsither/tibetan_monolingual_A_merged_7_lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "merged_dataset = load_dataset('spsither/tibetan_monolingual_A_merged_7_lines', cache_dir=\"/home/ec2-user/SageMaker/cache/datasets\", num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_dataset.save_to_disk('merged_6_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# merged_dataset = load_from_disk('merged_6_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what ever value you see for vocab_size, consider using that for VOCAB_SIZE. i.e. the least required value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len=512):\n",
    "        self.df = pd.DataFrame(dataset['text'])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inputs = self.tokenizer.encode_plus(self.df.iloc[i, 0],\n",
    "                                       max_length=self.max_len,\n",
    "                                       truncation=True,\n",
    "                                       padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "\n",
    "        return {'input_ids': inputs.input_ids[0], 'attention_mask': inputs.attention_mask[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset = CustomDataset(merged_dataset['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(merged_dataset['train'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'RoBERTa',\n",
    "    overwrite_output_dir = False,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    num_train_epochs = 50,         # number of epochs to train (default: 10)\n",
    "    learning_rate = 1e-4,          # learning rate (default: 0.001)\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size = 24, # 32 is too big. 24 is hitting 78.4% GPU memory usage\n",
    "    per_device_eval_batch_size  = 24, # can be larger than per_device_train_batch_size, no need for grad\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 100,\n",
    "    save_total_limit = 40,\n",
    "    report_to = ['wandb'],\n",
    ")\n",
    "# Create the trainer for our model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=eval_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save our tokenizer and create a model card\n",
    "repository_id = 'spsither/tibetan-RoBERTa'\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.config.to_json_file(f\"{tokenizer_folder}/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=f\"{model_folder}/checkpoint-110120\",\n",
    "    tokenizer=tokenizer_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          སེམས་ཀྱི་རང་བཞིན་འོད་གསལ་བ་ཟེར་ཡ་དེ་\n",
    "fill_mask(\"སེམས་ཀྱི་རང་བཞིནའོད་<mask>་བ་ཟེར་ཡ་དེ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = \"\"\"རིན་ <mask>\n",
    "ཆོས་ཀྱི་ <mask>\n",
    "རྫོགས་པའི་ <mask>\n",
    "གངས་རིའི་ <mask>\n",
    "མེ་ལོང་ <mask>\n",
    "བདེན་པའི་ <mask>\n",
    "'འབྱུང་ <mask>\"\"\".splitlines()\n",
    "\n",
    "for idx, sample in enumerate(samples, start=1):\n",
    "    outputs = fill_mask(sample)\n",
    "    print(idx, sample)\n",
    "    for output in outputs:\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"དེ་ནས་ཤར་ཕྱོགས་སུ་ནགས་སྟུག་པོ་བརྒྱུད་དེ་རྒྱང་གྲགས་ཉིས་བརྒྱ་བགྲོད་པ་ན་ཨི་ར་ན་བྷ་ཏའི་ཡུལ་ལོ། །ཐ་གྲུར་རྒྱང་གྲགས་སུམ་སྟོང་ལྷག་པ། ལྟེ་བའི་མཁར་ཆེན་ནི།\"\n",
    "ground = text.split('་')\n",
    "corrects = []\n",
    "for i in range(len(ground)):\n",
    "    test = ground[::]\n",
    "    mask = test[i]\n",
    "    test[i] = '<mask>'\n",
    "    test = '་'.join(test)\n",
    "    infs = fill_mask(test)\n",
    "    correct = False\n",
    "    for inf in infs:\n",
    "        if inf['token_str'] == mask:\n",
    "            correct = True\n",
    "    corrects += [correct]\n",
    "    \n",
    "print(sum(corrects), len(corrects))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
